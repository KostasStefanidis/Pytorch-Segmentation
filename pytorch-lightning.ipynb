{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.segmentation.deeplabv3 import deeplabv3_mobilenet_v3_large, DeepLabV3_MobileNet_V3_Large_Weights\n",
    "from torchvision.models.segmentation.deeplabv3 import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights\n",
    "from torchvision.models.segmentation.deeplabv3 import deeplabv3_resnet101, DeepLabV3_ResNet101_Weights\n",
    "from torch.optim import Adam, SGD, LBFGS, Adadelta, Adamax, Adagrad, ASGD\n",
    "from torch.optim.lr_scheduler import CyclicLR, PolynomialLR, CosineAnnealingWarmRestarts\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, ConstantLR, StepLR, CosineAnnealingLR\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, StochasticWeightAveraging, EarlyStopping\n",
    "from lightning.pytorch.callbacks import ModelSummary, LearningRateFinder, TQDMProgressBar, DeviceStatsMonitor\n",
    "from lightning.pytorch.profilers import AdvancedProfiler, PyTorchProfiler\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import sys\n",
    "#from utils.losses import IoULoss, DiceLoss, TverskyLoss, FocalTverskyLoss, HybridLoss, FocalHybridLoss\n",
    "from utils.datasets import CityscapesDataModule #, MapillaryDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "#from utils.eval import MeanIoU\n",
    "#from utils.models import  Unet, Residual_Unet, Attention_Unet, Unet_plus, DeepLabV3plus\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read YAML file\n",
    "print('Reading configuration from config yaml')\n",
    "\n",
    "with open('config/Cityscapes.yaml', 'r') as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "\n",
    "# TODO: Add default values if a variable is not defined in the config file\n",
    "\n",
    "LOGS_DIR = config.get('logs_dir')\n",
    "model_config = config.get('model_config')\n",
    "dataset_config = config.get('dataset_config')\n",
    "train_config = config.get('train_config')\n",
    "\n",
    "# Dataset Configuration\n",
    "DATASET = dataset_config.get('name')\n",
    "NUM_TRAIN_BATCHES = dataset_config.get('num_train_batches', 1.0)\n",
    "NUM_EVAL_BATCHES = dataset_config.get('num_eval_batches', 1.0)\n",
    "BATCH_SIZE = dataset_config.get('batch_size') #\n",
    "SEED = dataset_config.get('seed')\n",
    "\n",
    "# Model Configuration\n",
    "MODEL_TYPE = model_config.get('architecture')\n",
    "MODEL_NAME = model_config.get('name')\n",
    "BACKBONE = model_config.get('backbone')\n",
    "UNFREEZE_AT = model_config.get('unfreeze_at')\n",
    "INPUT_SHAPE = model_config.get('input_shape')\n",
    "OUTPUT_STRIDE = model_config.get('output_stride')\n",
    "FILTERS = model_config.get('filters')\n",
    "ACTIVATION = model_config.get('activation')\n",
    "DROPOUT_RATE = model_config.get('dropout_rate')\n",
    "\n",
    "# Training Configuration\n",
    "# PRETRAINED_WEIGHTS = model_config['pretrained_weights']\n",
    "\n",
    "\n",
    "EPOCHS = train_config.get('epochs') #\n",
    "AUGMENTATION = train_config.get('augment') #\n",
    "PRECISION = str(train_config.get('precision')) #\n",
    "\n",
    "# Stohastic weight averaging parameters\n",
    "SWA = train_config.get('swa')\n",
    "if SWA is not None:\n",
    "    SWA_LRS = SWA.get('lr', 1e-3)\n",
    "    SWA_EPOCH_START = SWA.get('epoch_start', 0.7)\n",
    "\n",
    "DISTRIBUTE_STRATEGY = train_config.get('distribute').get('strategy') #\n",
    "DEVICES = train_config.get('distribute').get('devices') #\n",
    "\n",
    "# save the config in the hparams.yaml file \n",
    "# with open(f'{LOGS_DIR}/my.yaml', 'w') as config_file:\n",
    "#     config = yaml.safe_dump(config, config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class DeepLabV3(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 model: nn.Module,\n",
    "                 train_config: dict = None\n",
    "                 ) -> None:        \n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore='model')\n",
    "        \n",
    "        loss = train_config.get('loss', 'CrossEntropy')\n",
    "        \n",
    "        self.model = model\n",
    "        self.loss = self.get_loss(loss)\n",
    "        self.optimizer_config = train_config.get('optimizer')\n",
    "        self.lr_schedule_config = train_config.get('lr_schedule')\n",
    "        self.batch_size = train_config.get('batch_size')\n",
    "        \n",
    "        #self.example_input_array = torch.Tensor(4, 3, 1024, 2048)\n",
    "        \n",
    "    def get_lr_schedule(self, optimizer):\n",
    "        lr = self.optimizer_config.get('learnin_rate', 1e-3)\n",
    "        schedule = self.lr_schedule_config.get('name')\n",
    "        \n",
    "        # num of steps in cyclic lr should be cycle_epochs * steps_per_epoch\n",
    "        # steps_per_epoch is defined depended on the length of the dataset\n",
    "        # so maybe define the dataset inside the Lightning Module using \n",
    "        # the DataModule object\n",
    "        \n",
    "        if schedule in ['Polynomial', 'PolynomialLr', 'PolynomialLR', 'polynomial']:\n",
    "            decay_epochs = self.lr_schedule_config.get('decay_epochs')\n",
    "            power = self.lr_schedule_config.get('power')\n",
    "            lr_schedule = PolynomialLR(\n",
    "                optimizer=optimizer,\n",
    "                total_iters=decay_epochs, #*steps_per_epoch,\n",
    "                power=power,\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "        elif schedule in ['CyclicLR', 'Cyclic', 'CyclicLr', 'cyclic']:\n",
    "            lr_schedule = CyclicLR(\n",
    "                optimizer = optimizer,\n",
    "                base_lr = lr,\n",
    "                max_lr = self.lr_schedule_config.get('max_lr', 1e-2),\n",
    "                # step_size_up=\n",
    "                # step_size_down=\n",
    "                gamma = self.lr_schedule_config.get('gamma', 1.0),\n",
    "                verbose=  True\n",
    "            )\n",
    "\n",
    "        return lr_schedule\n",
    "    \n",
    "    \n",
    "    def get_loss(self, loss: str):\n",
    "        if loss in ['CrossEntropy', 'CrossEntropyLoss', 'crossentropy']:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "        # elif loss in ['Dice, DiceLoss']:\n",
    "        #     loss_fn = DiceLoss()\n",
    "        # elif loss in ['Hybrid', 'HybridLoss']:\n",
    "        #     loss_fn = HybridLoss()\n",
    "        # elif loss in ['rmi', 'RMI', 'RmiLoss', 'RMILoss']:\n",
    "        #     loss_fn = RMILoss()\n",
    "        return loss_fn\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        input, target = train_batch\n",
    "        pred = self.model(input)['out']\n",
    "        loss = self.loss(pred, target)\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, on_step=False, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        input, target = val_batch\n",
    "        pred = self.model(input)['out']\n",
    "        loss = self.loss(pred, target)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, on_step=False, prog_bar=True, sync_dist=True)\n",
    "    \n",
    "    def predict_step(self, predict_batch: Any, batch_idx: int, dataloader_idx: int = 0) -> Any:\n",
    "        input = predict_batch\n",
    "        #filename = predict_batch['filename']\n",
    "        pred = self.model(input)['out']\n",
    "        return pred\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer_name = self.optimizer_config.get('name', 'Adam')\n",
    "        lr = self.optimizer_config.get('learnin_rate', 1e-3)\n",
    "        weight_decay = self.optimizer_config.get('weight_decay', 0)\n",
    "        momentum = self.optimizer_config.get('momentum', 0)\n",
    "        \n",
    "        optimizer_dict = {\n",
    "            'Adam' : Adam(params=self.model.parameters(),\n",
    "                          lr=lr,\n",
    "                          weight_decay=weight_decay),\n",
    "            'Adadelta' : Adadelta(params=self.model.parameters(),\n",
    "                                  lr=lr,\n",
    "                                  weight_decay=weight_decay),\n",
    "            'SGD' : SGD(params=self.model.parameters(),\n",
    "                        lr=lr,\n",
    "                        momentum=momentum,\n",
    "                        weight_decay=weight_decay)\n",
    "        }\n",
    "\n",
    "        optimizer = optimizer_dict[optimizer_name]\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': self.get_lr_schedule(optimizer)\n",
    "        }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_path = f'saved_models/{MODEL_TYPE}/{MODEL_NAME}'\n",
    "model_checkpoint_callback = ModelCheckpoint(dirpath=LOGS_DIR,\n",
    "                                            filename=model_checkpoint_path,\n",
    "                                            save_weights_only=False,\n",
    "                                            monitor='val_loss',\n",
    "                                            mode='min',\n",
    "                                #    monitor='MeanIoU',\n",
    "                                #    mode='max',\n",
    "                                   verbose=True)\n",
    "\n",
    "early_stopping_callback = EarlyStopping(patience=6,\n",
    "                                        monitor='val_loss',\n",
    "                                        # mode='max',\n",
    "                                        min_delta=1e-6,\n",
    "                                        verbose=True,\n",
    "                                        strict=True,\n",
    "                                        check_finite=True,\n",
    "                                        log_rank_zero_only=True)\n",
    "\n",
    "\n",
    "profiler = PyTorchProfiler(dirpath=LOGS_DIR, filename=\"perf-logs\")\n",
    "#profiler = AdvancedProfiler(dirpath=LOGS_DIR, filename=\"perf_logs\")\n",
    "#lr_finder_callback = LearningRateFinder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [model_checkpoint_callback, ModelSummary(max_depth=3), DeviceStatsMonitor()]\n",
    "\n",
    "if SWA is not None:\n",
    "    swa_callback = StochasticWeightAveraging(swa_lrs=SWA_LRS,\n",
    "                                         swa_epoch_start=SWA_EPOCH_START)\n",
    "    callbacks.append(swa_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(save_dir=f'{LOGS_DIR}/Tensorboard_logs', name=f'{MODEL_TYPE}/{MODEL_NAME}', log_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepLabV3(\n",
    "    model = deeplabv3_mobilenet_v3_large(num_classes=20),\n",
    "    train_config=train_config\n",
    ")\n",
    "\n",
    "data_module = CityscapesDataModule(dataset_config)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=DEVICES,\n",
    "    limit_train_batches=NUM_TRAIN_BATCHES,\n",
    "    limit_val_batches=NUM_EVAL_BATCHES,\n",
    "    max_epochs=EPOCHS,\n",
    "    precision=PRECISION,\n",
    "    deterministic=False,\n",
    "    callbacks=callbacks,\n",
    "    default_root_dir=LOGS_DIR,\n",
    "    logger=logger,\n",
    "    #strategy=DISTRIBUTE_STRATEGY\n",
    "    profiler='simple',\n",
    "    #sync_batchnorm=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.predict(model, datamodule=data_module, return_predictions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, datamodule=data_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
