{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter, FileWriter\n",
    "from torchvision.models.segmentation.deeplabv3 import deeplabv3_mobilenet_v3_large, DeepLabV3_MobileNet_V3_Large_Weights\n",
    "from torchvision.models.segmentation.deeplabv3 import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights\n",
    "from torchvision.models.segmentation.deeplabv3 import deeplabv3_resnet101, DeepLabV3_ResNet101_Weights\n",
    "from torch.optim import Adam, SGD, LBFGS, Adadelta, Adamax, Adagrad, ASGD\n",
    "from torch.optim.lr_scheduler import CyclicLR, PolynomialLR, CosineAnnealingWarmRestarts\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, ConstantLR, StepLR, CosineAnnealingLR\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, StochasticWeightAveraging, EarlyStopping\n",
    "from lightning.pytorch.callbacks import ModelSummary, LearningRateFinder, TQDMProgressBar, DeviceStatsMonitor\n",
    "from lightning.pytorch.profilers import AdvancedProfiler, PyTorchProfiler\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import sys\n",
    "#from utils.losses import IoULoss, DiceLoss, TverskyLoss, FocalTverskyLoss, HybridLoss, FocalHybridLoss\n",
    "from utils.datasets import CityscapesDataset #, MapillaryDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "#from utils.eval import MeanIoU\n",
    "#from utils.models import  Unet, Residual_Unet, Attention_Unet, Unet_plus, DeepLabV3plus\n",
    "from argparse import ArgumentParser\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading configuration from config yaml\n"
     ]
    }
   ],
   "source": [
    "# Read YAML file\n",
    "print('Reading configuration from config yaml')\n",
    "\n",
    "with open('config/Cityscapes.yaml', 'r') as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "\n",
    "# TODO: Add default values if a variable is not defined in the config file\n",
    "\n",
    "LOGS_DIR = config.get('logs_dir')\n",
    "\n",
    "model_config = config.get('model_config')\n",
    "dataset_config = config.get('dataset_config')\n",
    "train_config = config.get('train_config')\n",
    "\n",
    "# Dataset Configuration\n",
    "DATASET = dataset_config.get('name')\n",
    "DATA_PATH = dataset_config.get('path')\n",
    "VERSION = dataset_config.get('version')\n",
    "NUM_TRAIN_IMAGES = dataset_config.get('num_train_images')\n",
    "NUM_EVAL_IMAGES = dataset_config.get('num_eval_images')\n",
    "SEED = dataset_config.get('seed')\n",
    "\n",
    "# Model Configuration\n",
    "MODEL_TYPE = model_config.get('architecture')\n",
    "MODEL_NAME = model_config.get('name')\n",
    "BACKBONE = model_config.get('backbone')\n",
    "UNFREEZE_AT = model_config.get('unfreeze_at')\n",
    "INPUT_SHAPE = model_config.get('input_shape')\n",
    "OUTPUT_STRIDE = model_config.get('output_stride')\n",
    "FILTERS = model_config.get('filters')\n",
    "ACTIVATION = model_config.get('activation')\n",
    "DROPOUT_RATE = model_config.get('dropout_rate')\n",
    "\n",
    "# Training Configuration\n",
    "# PRETRAINED_WEIGHTS = model_config['pretrained_weights']\n",
    "\n",
    "BATCH_SIZE = train_config.get('batch_size') #\n",
    "EPOCHS = train_config.get('epochs') #\n",
    "AUGMENTATION = train_config.get('augment') #\n",
    "PRECISION = str(train_config.get('precision')) #\n",
    "\n",
    "# Stohastic weight averaging parameters\n",
    "SWA = train_config.get('swa')\n",
    "if SWA is not None:\n",
    "    SWA_LRS = SWA.get('lr', 1e-3)\n",
    "    SWA_EPOCH_START = SWA.get('epoch_start', 0.7)\n",
    "\n",
    "DISTRIBUTE_STRATEGY = train_config.get('distribute').get('strategy') #\n",
    "DEVICES = train_config.get('distribute').get('devices') #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabV3(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 model: nn.Module,\n",
    "                 train_config: dict = None\n",
    "                 \n",
    "                 ) -> None:        \n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore='model')\n",
    "        \n",
    "        loss = train_config.get('loss', 'CrossEntropy')\n",
    "        \n",
    "        self.model = model\n",
    "        self.loss = self.get_loss(loss)\n",
    "        self.optimizer_config = train_config.get('optimizer')\n",
    "        self.lr_schedule_config = train_config.get('lr_schedule')\n",
    "        self.batch_size = train_config.get('batch_size')\n",
    "        \n",
    "        #self.example_input_array = torch.Tensor(self.batch_size, 3, 1024, 2048)\n",
    "        \n",
    "    def get_lr_schedule(self, optimizer):\n",
    "        lr = self.optimizer_config.get('learnin_rate', 1e-3)\n",
    "        schedule = self.lr_schedule_config.get('name')\n",
    "        \n",
    "        # num of steps in cyclic lr should be cycle_epochs * steps_per_epoch\n",
    "        # steps_per_epoch is defined depended on the length of the dataset\n",
    "        # so maybe define the dataset inside the Lightning Module using \n",
    "        # the DataModule object\n",
    "        \n",
    "        if schedule in ['Polynomial', 'PolynomialLr', 'PolynomialLR', 'polynomial']:\n",
    "            decay_epochs = self.lr_schedule_config.get('decay_epochs')\n",
    "            power = self.lr_schedule_config.get('power')\n",
    "            lr_schedule = PolynomialLR(\n",
    "                optimizer=optimizer,\n",
    "                total_iters=decay_epochs, #*steps_per_epoch,\n",
    "                power=power,\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "        elif schedule in ['CyclicLR', 'Cyclic', 'CyclicLr', 'cyclic']:\n",
    "            lr_schedule = CyclicLR(\n",
    "                optimizer = optimizer,\n",
    "                base_lr = lr,\n",
    "                max_lr = self.lr_schedule_config.get('max_lr', 1e-2),\n",
    "                # step_size_up=\n",
    "                # step_size_down=\n",
    "                gamma = self.lr_schedule_config.get('gamma', 1.0),\n",
    "                verbose=  True\n",
    "            )\n",
    "\n",
    "        return lr_schedule\n",
    "    \n",
    "    \n",
    "    def get_loss(self, loss: str):\n",
    "        if loss in ['CrossEntropy', 'CrossEntropyLoss', 'crossentropy']:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "        # elif loss in ['Dice, DiceLoss']:\n",
    "        #     loss_fn = DiceLoss()\n",
    "        # elif loss in ['Hybrid', 'HybridLoss']:\n",
    "        #     loss_fn = HybridLoss()\n",
    "        # elif loss in ['rmi', 'RMI', 'RmiLoss', 'RMILoss']:\n",
    "        #     loss_fn = RMILoss()\n",
    "        return loss_fn\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        input, target = train_batch\n",
    "        pred = self.model(input)['out']\n",
    "        loss = self.loss(pred, target)\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, on_step=False, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        input, target = val_batch\n",
    "        pred = self.model(input)['out']\n",
    "        loss = self.loss(pred, target)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, on_step=False, prog_bar=True, sync_dist=True)\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer_name = self.optimizer_config.get('name', 'Adam')\n",
    "        lr = self.optimizer_config.get('learnin_rate', 1e-3)\n",
    "        weight_decay = self.optimizer_config.get('weight_decay', 0)\n",
    "        momentum = self.optimizer_config.get('momentum', 0)\n",
    "        \n",
    "        optimizer_dict = {\n",
    "            'Adam' : Adam(params=self.model.parameters(),\n",
    "                          lr=lr,\n",
    "                          weight_decay=weight_decay),\n",
    "            'Adadelta' : Adadelta(params=self.model.parameters(),\n",
    "                                  lr=lr,\n",
    "                                  weight_decay=weight_decay),\n",
    "            'SGD' : SGD(params=self.model.parameters(),\n",
    "                        lr=lr,\n",
    "                        momentum=momentum,\n",
    "                        weight_decay=weight_decay)\n",
    "        }\n",
    "\n",
    "        optimizer = optimizer_dict[optimizer_name]\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': self.get_lr_schedule(optimizer)\n",
    "        }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = CityscapesDataset(root=DATA_PATH, \n",
    "                             split='train', \n",
    "                             mode=VERSION, \n",
    "                             target_type='semantic'\n",
    "                             )\n",
    "train_loader = DataLoader(dataset=train_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "val_ds = CityscapesDataset(root=DATA_PATH, \n",
    "                           split='val', \n",
    "                           mode=VERSION, \n",
    "                           target_type='semantic'\n",
    "                           )\n",
    "val_loader = DataLoader(dataset=val_ds, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_path = f'saved_models/{MODEL_TYPE}/{MODEL_NAME}'\n",
    "model_checkpoint_callback = ModelCheckpoint(dirpath=LOGS_DIR,\n",
    "                                            filename=model_checkpoint_path,\n",
    "                                            save_weights_only=False,\n",
    "                                            monitor='val_loss',\n",
    "                                            mode='min',\n",
    "                                #    monitor='MeanIoU',\n",
    "                                #    mode='max',\n",
    "                                   verbose=True)\n",
    "\n",
    "early_stopping_callback = EarlyStopping(patience=6,\n",
    "                                        monitor='val_loss',\n",
    "                                        # mode='max',\n",
    "                                        min_delta=1e-6,\n",
    "                                        verbose=True,\n",
    "                                        strict=True,\n",
    "                                        check_finite=True,\n",
    "                                        log_rank_zero_only=True)\n",
    "\n",
    "\n",
    "profiler = PyTorchProfiler(dirpath=LOGS_DIR, filename=\"perf-logs\")\n",
    "#profiler = AdvancedProfiler(dirpath=LOGS_DIR, filename=\"perf_logs\")\n",
    "#lr_finder_callback = LearningRateFinder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [model_checkpoint_callback, ModelSummary(max_depth=3), DeviceStatsMonitor()]\n",
    "\n",
    "if SWA is not None:\n",
    "    swa_callback = StochasticWeightAveraging(swa_lrs=SWA_LRS,\n",
    "                                         swa_epoch_start=SWA_EPOCH_START)\n",
    "    callbacks.append(swa_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(save_dir=LOGS_DIR, name='Tensorboard_logs', version=f'{MODEL_TYPE}/{MODEL_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model = DeepLabV3(\n",
    "    model = deeplabv3_mobilenet_v3_large(num_classes=20),\n",
    "    train_config=train_config\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=DEVICES,\n",
    "    limit_train_batches=NUM_TRAIN_IMAGES, \n",
    "    limit_val_batches=NUM_EVAL_IMAGES,\n",
    "    max_epochs=EPOCHS,\n",
    "    precision=PRECISION,\n",
    "    deterministic=False,\n",
    "    callbacks=callbacks,\n",
    "    default_root_dir=LOGS_DIR,\n",
    "    logger=logger,\n",
    "    #strategy=DISTRIBUTE_STRATEGY\n",
    "    profiler='simple',\n",
    "    #sync_batchnorm=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kstef/.local/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /mnt/logs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name               | Type                    | Params\n",
      "----------------------------------------------------------------\n",
      "0  | model              | DeepLabV3               | 11.0 M\n",
      "1  | model.backbone     | IntermediateLayerGetter | 3.0 M \n",
      "2  | model.backbone.0   | Conv2dNormActivation    | 464   \n",
      "3  | model.backbone.1   | InvertedResidual        | 464   \n",
      "4  | model.backbone.2   | InvertedResidual        | 3.4 K \n",
      "5  | model.backbone.3   | InvertedResidual        | 4.4 K \n",
      "6  | model.backbone.4   | InvertedResidual        | 10.3 K\n",
      "7  | model.backbone.5   | InvertedResidual        | 21.0 K\n",
      "8  | model.backbone.6   | InvertedResidual        | 21.0 K\n",
      "9  | model.backbone.7   | InvertedResidual        | 32.1 K\n",
      "10 | model.backbone.8   | InvertedResidual        | 34.8 K\n",
      "11 | model.backbone.9   | InvertedResidual        | 32.0 K\n",
      "12 | model.backbone.10  | InvertedResidual        | 32.0 K\n",
      "13 | model.backbone.11  | InvertedResidual        | 214 K \n",
      "14 | model.backbone.12  | InvertedResidual        | 386 K \n",
      "15 | model.backbone.13  | InvertedResidual        | 429 K \n",
      "16 | model.backbone.14  | InvertedResidual        | 797 K \n",
      "17 | model.backbone.15  | InvertedResidual        | 797 K \n",
      "18 | model.backbone.16  | Conv2dNormActivation    | 155 K \n",
      "19 | model.classifier   | DeepLabHead             | 8.1 M \n",
      "20 | model.classifier.0 | ASPP                    | 7.5 M \n",
      "21 | model.classifier.1 | Conv2d                  | 589 K \n",
      "22 | model.classifier.2 | BatchNorm2d             | 512   \n",
      "23 | model.classifier.3 | ReLU                    | 0     \n",
      "24 | model.classifier.4 | Conv2d                  | 5.1 K \n",
      "25 | loss               | CrossEntropyLoss        | 0     \n",
      "----------------------------------------------------------------\n",
      "11.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.0 M    Total params\n",
      "44.101    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kstef/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/mnt/Pytorch-Segmentation/utils/datasets/CityscapesUtils.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  one_hot_output.scatter_(0, torch.tensor(input, dtype=torch.int64), 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-05-06 12:19:40 196158:196158 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   8%|▊         | 4/50 [00:04<00:55,  1.21s/it, v_num=ets1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-05-06 12:19:48 196158:196158 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-06 12:19:48 196158:196158 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "[W collection.cpp:496] Warning: Optimizer.step#Adam.step (function operator())\n",
      "[W collection.cpp:496] Warning: [pl][profile][LightningModule]DeepLabV3.optimizer_step (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  98%|█████████▊| 49/50 [00:40<00:00,  1.21it/s, v_num=ets1]Adjusting learning rate of group 0 to 8.1000e-04.\n",
      "Epoch 0: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, v_num=ets1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-05-06 12:20:25 196158:196158 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-05-06 12:20:27 196158:196158 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-06 12:20:27 196158:196158 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 50/50 [00:49<00:00,  1.01it/s, v_num=ets1, val_loss=1.290, train_loss=0.884]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 50: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  98%|█████████▊| 49/50 [00:39<00:00,  1.23it/s, v_num=ets1, val_loss=1.290, train_loss=0.884]Adjusting learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 1: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, v_num=ets1, val_loss=1.180, train_loss=0.588]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 100: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  98%|█████████▊| 49/50 [00:39<00:00,  1.23it/s, v_num=ets1, val_loss=1.180, train_loss=0.588]Adjusting learning rate of group 0 to 4.9000e-04.\n",
      "Epoch 2: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, v_num=ets1, val_loss=1.360, train_loss=0.478]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 150: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  98%|█████████▊| 49/50 [00:39<00:00,  1.23it/s, v_num=ets1, val_loss=1.360, train_loss=0.478]Adjusting learning rate of group 0 to 3.6000e-04.\n",
      "Epoch 3: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, v_num=ets1, val_loss=1.130, train_loss=0.403]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 200: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  98%|█████████▊| 49/50 [00:39<00:00,  1.23it/s, v_num=ets1, val_loss=1.130, train_loss=0.403]Adjusting learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 4: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, v_num=ets1, val_loss=1.020, train_loss=0.340]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, v_num=ets1, val_loss=1.020, train_loss=0.340]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
